
  0%|                                                                                                                                 | 0/30000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)



 33%|██████████████████████████████████████▋                                                                             | 10005/30000 [00:28<01:22, 242.88it/s]


 33%|███████████████████████████████████████                                                                              | 10009/30000 [00:51<03:39, 91.05it/s]




 33%|███████████████████████████████████████                                                                              | 10015/30000 [01:27<10:45, 30.94it/s]



 33%|███████████████████████████████████████                                                                              | 10020/30000 [01:57<23:42, 14.04it/s]


 33%|███████████████████████████████████████                                                                              | 10024/30000 [02:19<41:24,  8.04it/s]




 33%|██████████████████████████████████████▍                                                                            | 10028/30000 [02:43<1:19:48,  4.17it/s]







 33%|██████████████████████████████████████▍                                                                            | 10035/30000 [03:23<4:25:21,  1.25it/s]



 33%|██████████████████████████████████████▍                                                                            | 10039/30000 [03:45<8:13:11,  1.48s/it]





 33%|██████████████████████████████████████▏                                                                           | 10044/30000 [04:14<15:41:41,  2.83s/it]





 34%|██████████████████████████████████████▏                                                                           | 10050/30000 [04:50<26:51:43,  4.85s/it]




 34%|██████████████████████████████████████▏                                                                           | 10054/30000 [05:14<30:49:05,  5.56s/it]





 34%|██████████████████████████████████████▏                                                                           | 10059/30000 [05:44<32:29:49,  5.87s/it]






 34%|██████████████████████████████████████▏                                                                           | 10065/30000 [06:18<31:15:45,  5.65s/it]





 34%|██████████████████████████████████████▎                                                                           | 10070/30000 [06:48<32:45:31,  5.92s/it]




 34%|██████████████████████████████████████▎                                                                           | 10074/30000 [07:12<33:12:31,  6.00s/it]






 34%|██████████████████████████████████████▎                                                                           | 10080/30000 [07:46<31:42:42,  5.73s/it]





 34%|██████████████████████████████████████▎                                                                           | 10085/30000 [08:17<34:30:26,  6.24s/it]





 34%|██████████████████████████████████████▎                                                                           | 10090/30000 [08:47<32:53:41,  5.95s/it]





 34%|██████████████████████████████████████▎                                                                           | 10095/30000 [09:15<31:37:40,  5.72s/it]




 34%|██████████████████████████████████████▍                                                                           | 10099/30000 [09:39<32:47:04,  5.93s/it]





 34%|██████████████████████████████████████▍                                                                           | 10104/30000 [10:07<32:23:40,  5.86s/it]





 34%|██████████████████████████████████████▍                                                                           | 10109/30000 [10:38<33:07:49,  6.00s/it]






 34%|██████████████████████████████████████▍                                                                           | 10115/30000 [11:15<33:24:35,  6.05s/it]





 34%|██████████████████████████████████████▍                                                                           | 10120/30000 [11:44<33:01:32,  5.98s/it]





 34%|██████████████████████████████████████▍                                                                           | 10125/30000 [12:13<32:04:30,  5.81s/it]





 34%|██████████████████████████████████████▍                                                                           | 10130/30000 [12:43<33:08:46,  6.01s/it]





 34%|██████████████████████████████████████▌                                                                           | 10135/30000 [13:11<32:27:41,  5.88s/it]





 34%|██████████████████████████████████████▌                                                                           | 10140/30000 [13:41<32:02:05,  5.81s/it]





 34%|██████████████████████████████████████▌                                                                           | 10145/30000 [14:11<32:25:40,  5.88s/it]




 34%|██████████████████████████████████████▌                                                                           | 10149/30000 [14:35<32:45:10,  5.94s/it]






 34%|██████████████████████████████████████▌                                                                           | 10155/30000 [15:09<32:15:19,  5.85s/it]





 34%|██████████████████████████████████████▌                                                                           | 10160/30000 [15:37<30:52:59,  5.60s/it]





 34%|██████████████████████████████████████▋                                                                           | 10165/30000 [16:07<32:24:56,  5.88s/it]





 34%|██████████████████████████████████████▋                                                                           | 10170/30000 [16:37<32:55:46,  5.98s/it]





 34%|██████████████████████████████████████▋                                                                           | 10175/30000 [17:07<32:52:27,  5.97s/it]





 34%|██████████████████████████████████████▋                                                                           | 10180/30000 [17:37<32:46:57,  5.95s/it]




 34%|██████████████████████████████████████▋                                                                           | 10184/30000 [18:01<33:08:28,  6.02s/it]






 34%|██████████████████████████████████████▋                                                                           | 10190/30000 [18:36<32:01:52,  5.82s/it]





 34%|██████████████████████████████████████▋                                                                           | 10195/30000 [19:06<32:38:35,  5.93s/it]





 34%|██████████████████████████████████████▊                                                                           | 10200/30000 [19:34<31:32:27,  5.73s/it]




 34%|██████████████████████████████████████▊                                                                           | 10204/30000 [19:57<31:30:02,  5.73s/it]






 34%|██████████████████████████████████████▊                                                                           | 10210/30000 [20:32<32:22:12,  5.89s/it]




 34%|██████████████████████████████████████▊                                                                           | 10214/30000 [20:54<31:46:23,  5.78s/it]





 34%|██████████████████████████████████████▊                                                                           | 10219/30000 [21:24<33:05:37,  6.02s/it]





 34%|██████████████████████████████████████▊                                                                           | 10224/30000 [21:55<33:06:29,  6.03s/it]






 34%|██████████████████████████████████████▊                                                                           | 10230/30000 [22:30<32:15:15,  5.87s/it]





 34%|██████████████████████████████████████▉                                                                           | 10235/30000 [22:58<31:09:53,  5.68s/it]




 34%|██████████████████████████████████████▉                                                                           | 10239/30000 [23:21<31:15:48,  5.70s/it]





 34%|██████████████████████████████████████▉                                                                           | 10244/30000 [23:50<32:28:32,  5.92s/it]Traceback (most recent call last):
  File "/home/jingbo/KVMemory/finetune_combine.py", line 91, in <module>
    main()
  File "/home/jingbo/KVMemory/finetune_combine.py", line 85, in main
    trainer.train(resume_from_checkpoint = True)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/jingbo/KVMemory/src/training/trainer.py", line 332, in compute_loss
    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels = input_ids)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/peft_model.py", line 1430, in forward
    return self.base_model(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1159, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/jingbo/KVMemory/finetune_combine.py", line 91, in <module>
[rank0]:     main()
[rank0]:   File "/home/jingbo/KVMemory/finetune_combine.py", line 85, in main
[rank0]:     trainer.train(resume_from_checkpoint = True)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:   File "/home/jingbo/KVMemory/src/training/trainer.py", line 332, in compute_loss
[rank0]:     outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels = input_ids)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/peft_model.py", line 1430, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1159, in forward
[rank0]:     logits = logits.float()
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU