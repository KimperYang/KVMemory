
  0%|                                          | 0/10000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
0
1
loss2:  tensor(0.7456, device='cuda:0', grad_fn=<DivBackward0>)
2
3
loss2:  tensor(0.8679, device='cuda:0', grad_fn=<DivBackward0>)
4
5
loss2:  tensor(0.7778, device='cuda:0', grad_fn=<DivBackward0>)
6
7
  0%|                               | 1/10000 [00:06<17:54:52,  6.45s/it]
loss2:  tensor(0.9844, device='cuda:0', grad_fn=<DivBackward0>)
8
9
loss2:  tensor(0.9039, device='cuda:0', grad_fn=<DivBackward0>)
10
11
loss2:  tensor(0.5428, device='cuda:0', grad_fn=<DivBackward0>)
12
13
loss2:  tensor(0.6307, device='cuda:0', grad_fn=<DivBackward0>)
14

  0%|                               | 2/10000 [00:12<17:04:33,  6.15s/it]
loss2:  tensor(0.7682, device='cuda:0', grad_fn=<DivBackward0>)
16
17
loss2:  tensor(0.6239, device='cuda:0', grad_fn=<DivBackward0>)
18
19
loss2:  tensor(0.8137, device='cuda:0', grad_fn=<DivBackward0>)
20
21
loss2:  tensor(0.6880, device='cuda:0', grad_fn=<DivBackward0>)
22

  0%|                               | 3/10000 [00:18<17:06:47,  6.16s/it]
loss2:  tensor(0.5712, device='cuda:0', grad_fn=<DivBackward0>)
24
25
loss2:  tensor(0.5814, device='cuda:0', grad_fn=<DivBackward0>)
26
27
loss2:  tensor(0.8011, device='cuda:0', grad_fn=<DivBackward0>)
28
29
loss2:  tensor(0.9298, device='cuda:0', grad_fn=<DivBackward0>)
30

  0%|                               | 4/10000 [00:24<16:53:36,  6.08s/it]
loss2:  tensor(0.7161, device='cuda:0', grad_fn=<DivBackward0>)
32
33
loss2:  tensor(0.6501, device='cuda:0', grad_fn=<DivBackward0>)
34
35
loss2:  tensor(0.6039, device='cuda:0', grad_fn=<DivBackward0>)
36
37
loss2:  tensor(1.0318, device='cuda:0', grad_fn=<DivBackward0>)
38

  0%|                               | 5/10000 [00:30<16:49:48,  6.06s/it]
loss2:  tensor(0.8048, device='cuda:0', grad_fn=<DivBackward0>)
{'loss': 1.4483, 'grad_norm': 0.19585217535495758, 'learning_rate': 1.9990000000000003e-05, 'epoch': 0.0}
40
41
loss2:  tensor(0.7819, device='cuda:0', grad_fn=<DivBackward0>)
42
43
loss2:  tensor(0.5228, device='cuda:0', grad_fn=<DivBackward0>)
44
45
loss2:  tensor(0.8142, device='cuda:0', grad_fn=<DivBackward0>)
46
47
loss2:  tensor(0.3939, device='cuda:0', grad_fn=<DivBackward0>)
48

  0%|                               | 6/10000 [00:36<17:02:27,  6.14s/it]
loss2:  tensor(0.9688, device='cuda:0', grad_fn=<DivBackward0>)
50
51
loss2:  tensor(0.7334, device='cuda:0', grad_fn=<DivBackward0>)
52
53

  0%|                               | 7/10000 [00:42<16:39:53,  6.00s/it]
54
55
loss2:  tensor(1.0095, device='cuda:0', grad_fn=<DivBackward0>)
56
57
loss2:  tensor(0.7519, device='cuda:0', grad_fn=<DivBackward0>)
58
59
loss2:  tensor(0.6971, device='cuda:0', grad_fn=<DivBackward0>)
60
61
loss2:  tensor(0.6030, device='cuda:0', grad_fn=<DivBackward0>)
62

  0%|                               | 8/10000 [00:48<16:43:54,  6.03s/it]
loss2:  tensor(0.9554, device='cuda:0', grad_fn=<DivBackward0>)
64
65
loss2:  tensor(0.5594, device='cuda:0', grad_fn=<DivBackward0>)
66
67
loss2:  tensor(0.6729, device='cuda:0', grad_fn=<DivBackward0>)
68
69
loss2:  tensor(0.8451, device='cuda:0', grad_fn=<DivBackward0>)
70

  0%|                               | 9/10000 [00:54<16:45:04,  6.04s/it]
loss2:  tensor(0.7817, device='cuda:0', grad_fn=<DivBackward0>)
72
73
loss2:  tensor(0.6958, device='cuda:0', grad_fn=<DivBackward0>)
74
75
loss2:  tensor(1.1604, device='cuda:0', grad_fn=<DivBackward0>)
76
77
loss2:  tensor(0.8983, device='cuda:0', grad_fn=<DivBackward0>)
78
79
loss2:  tensor(0.2114, device='cuda:0', grad_fn=<DivBackward0>)
{'loss': 1.4648, 'grad_norm': 0.2715247571468353, 'learning_rate': 1.9980000000000002e-05, 'epoch': 0.0}
80

  0%|                              | 10/10000 [01:00<16:52:05,  6.08s/it]
loss2:  tensor(0.9046, device='cuda:0', grad_fn=<DivBackward0>)
82
83
loss2:  tensor(0.4942, device='cuda:0', grad_fn=<DivBackward0>)
84
85
loss2:  tensor(0.7263, device='cuda:0', grad_fn=<DivBackward0>)
86
87
loss2:  tensor(0.6910, device='cuda:0', grad_fn=<DivBackward0>)
88

  0%|                              | 11/10000 [01:06<16:52:57,  6.08s/it]
loss2:  tensor(0.7949, device='cuda:0', grad_fn=<DivBackward0>)
90
91
loss2:  tensor(0.6645, device='cuda:0', grad_fn=<DivBackward0>)
92
93
loss2:  tensor(0.5750, device='cuda:0', grad_fn=<DivBackward0>)
94
95
loss2:  tensor(0.8472, device='cuda:0', grad_fn=<DivBackward0>)
96

  0%|                              | 12/10000 [01:12<16:45:14,  6.04s/it]
loss2:  tensor(0.8283, device='cuda:0', grad_fn=<DivBackward0>)
98
99
loss2:  tensor(0.5071, device='cuda:0', grad_fn=<DivBackward0>)
100
101
loss2:  tensor(0.5664, device='cuda:0', grad_fn=<DivBackward0>)
102
103

loss2:  tensor(0.5336, device='cuda:0', grad_fn=<DivBackward0>)
104
105
loss2:  tensor(0.8428, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7063, device='cuda:0', grad_fn=<DivBackward0>)
108
loss2:  tensor(0.7063, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.7441, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8339, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8339, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
123s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
127s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 16/10000 [01:36<16:18:39,  5.88s/it]
131s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
135s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 17/10000 [01:42<16:27:50,  5.94s/it]
139s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
143s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 18/10000 [01:48<16:20:49,  5.90s/it]
147s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
151s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 19/10000 [01:54<16:21:19,  5.90s/it]
155s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
159s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 20/10000 [02:00<16:25:03,  5.92s/it]
162s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
166s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 21/10000 [02:06<16:35:47,  5.99s/it]
170s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
174s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 22/10000 [02:12<16:30:09,  5.95s/it]
178s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
182s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 23/10000 [02:17<16:27:59,  5.94s/it]
186s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
190s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 24/10000 [02:24<16:39:32,  6.01s/it]
194s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)
198s2:  tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6531, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6531, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6801, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6779, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.0387, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.8805, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8805, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6000, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6486, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6486, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7877, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.5948, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5948, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)

241s2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)
245s2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)
245s2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)

249s2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)
252s2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)
252s2:  tensor(1.1888, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.4951, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4951, device='cuda:0', grad_fn=<DivBackward0>)
262s2:  tensor(0.4951, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6491, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6491, device='cuda:0', grad_fn=<DivBackward0>)
270s2:  tensor(0.6491, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6912, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6912, device='cuda:0', grad_fn=<DivBackward0>)
278s2:  tensor(0.6912, device='cuda:0', grad_fn=<DivBackward0>)

281s2:  tensor(0.6912, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6898, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6898, device='cuda:0', grad_fn=<DivBackward0>)

288s2:  tensor(0.6898, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9329, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9329, device='cuda:0', grad_fn=<DivBackward0>)

296s2:  tensor(0.9329, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7059, device='cuda:0', grad_fn=<DivBackward0>)
302s2:  tensor(0.7059, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.7771, device='cuda:0', grad_fn=<DivBackward0>)
308s2:  tensor(0.7771, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                              | 39/10000 [03:52<16:07:56,  5.83s/it]
316s2:  tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)
316s2:  tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)

321s2:  tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)
321s2:  tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)
326s2:  tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.3880, device='cuda:0', grad_fn=<DivBackward0>)
332s2:  tensor(0.3880, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7879, device='cuda:0', grad_fn=<DivBackward0>)

338s2:  tensor(0.7879, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7879, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6010, device='cuda:0', grad_fn=<DivBackward0>)

346s2:  tensor(0.6010, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7511, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7511, device='cuda:0', grad_fn=<DivBackward0>)

354s2:  tensor(0.7511, device='cuda:0', grad_fn=<DivBackward0>)
354s2:  tensor(0.7511, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7222, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6744, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6744, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.3640, device='cuda:0', grad_fn=<DivBackward0>)

370s2:  tensor(0.3640, device='cuda:0', grad_fn=<DivBackward0>)
370s2:  tensor(0.3640, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6402, device='cuda:0', grad_fn=<DivBackward0>)

378s2:  tensor(0.6402, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6402, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8184, device='cuda:0', grad_fn=<DivBackward0>)

386s2:  tensor(0.8184, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8184, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7118, device='cuda:0', grad_fn=<DivBackward0>)

394s2:  tensor(0.7118, device='cuda:0', grad_fn=<DivBackward0>)
394s2:  tensor(0.7118, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7038, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.4126, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4126, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6283, device='cuda:0', grad_fn=<DivBackward0>)

410s2:  tensor(0.6283, device='cuda:0', grad_fn=<DivBackward0>)
410s2:  tensor(0.6283, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.3644, device='cuda:0', grad_fn=<DivBackward0>)

  1%|▏                             | 52/10000 [05:10<16:43:31,  6.05s/it]
420s2:  tensor(0.3644, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6020, device='cuda:0', grad_fn=<DivBackward0>)

  1%|▏                             | 53/10000 [05:16<16:47:17,  6.08s/it]
  1%|▏                             | 53/10000 [05:16<16:47:17,  6.08s/it]Traceback (most recent call last):
  File "/home/jingbo/KVMemory/finetune_combine.py", line 94, in <module>
    main()
  File "/home/jingbo/KVMemory/finetune_combine.py", line 86, in main
    trainer.train()
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 2274, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 3313, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/jingbo/KVMemory/src/training/trainer.py", line 297, in compute_loss
    split_past_key_values = generate_kv_with_position(self.model, split_input_ids, position_ids = memory_position_batch)
  File "/home/jingbo/KVMemory/src/utils/cache.py", line 27, in generate_kv_with_position
    out = model(input_ids = input_ids, position_ids = position_ids)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/peft_model.py", line 1430, in forward
    return self.base_model(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1076, in forward
    outputs = self.model(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 880, in forward
    layer_outputs = decoder_layer(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 619, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 397, in forward
    query_states = self.q_proj(hidden_states)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 569, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/jingbo/KVMemory/finetune_combine.py", line 94, in <module>
[rank0]:     main()
[rank0]:   File "/home/jingbo/KVMemory/finetune_combine.py", line 86, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 2274, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 3313, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:   File "/home/jingbo/KVMemory/src/training/trainer.py", line 297, in compute_loss
[rank0]:     split_past_key_values = generate_kv_with_position(self.model, split_input_ids, position_ids = memory_position_batch)
[rank0]:   File "/home/jingbo/KVMemory/src/utils/cache.py", line 27, in generate_kv_with_position
[rank0]:     out = model(input_ids = input_ids, position_ids = position_ids)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/peft_model.py", line 1430, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1076, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 880, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 619, in forward
[rank0]:     hidden_states, self_attn_weights, present_key_value = self.self_attn(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 397, in forward
[rank0]:     query_states = self.q_proj(hidden_states)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 569, in forward
[rank0]:     result = result + lora_B(lora_A(dropout(x))) * scaling
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 116, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: KeyboardInterrupt
loss2:  tensor(0.6020, device='cuda:0', grad_fn=<DivBackward0>)