
  0%|                                                              | 0/10000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
0
1
loss1:  tensor(2.6467, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5686, device='cuda:0', grad_fn=<DivBackward0>)
2
3
loss1:  tensor(2.2493, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8086, device='cuda:0', grad_fn=<DivBackward0>)
4
5
loss1:  tensor(2.0436, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7504, device='cuda:0', grad_fn=<DivBackward0>)
6
7
loss1:
loss1:  tensor(2.0742, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6571, device='cuda:0', grad_fn=<DivBackward0>)
8
9
loss1:  tensor(2.1668, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7175, device='cuda:0', grad_fn=<DivBackward0>)
10
11
loss1:  tensor(1.1517, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6786, device='cuda:0', grad_fn=<DivBackward0>)
12
13
loss1:  tensor(2.1384, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8098, device='cuda:0', grad_fn=<DivBackward0>)
14
15
loss1:  tensor(1.9505, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.6242, device='cuda:0', grad_fn=<DivBackward0>)
16
17
loss1:  tensor(2.2919, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.8265, device='cuda:0', grad_fn=<DivBackward0>)
18
19
loss1:  tensor(1.9830, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6673, device='cuda:0', grad_fn=<DivBackward0>)
20
21
loss1:  tensor(1.4565, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6999, device='cuda:0', grad_fn=<DivBackward0>)
22
23
loss1:  tensor(2.2907, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5505, device='cuda:0', grad_fn=<DivBackward0>)
24

  0%|                                                   | 3/10000 [00:16<15:30:03,  5.58s/it]
loss1:  tensor(2.3111, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6322, device='cuda:0', grad_fn=<DivBackward0>)
26
27
loss1:  tensor(2.0495, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7401, device='cuda:0', grad_fn=<DivBackward0>)
28
29
loss1:  tensor(2.3658, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.1766, device='cuda:0', grad_fn=<DivBackward0>)
30
31

  0%|                                                   | 4/10000 [00:22<14:59:32,  5.40s/it]
loss2:  tensor(0.9684, device='cuda:0', grad_fn=<DivBackward0>)
32
33
loss1:  tensor(2.2308, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9533, device='cuda:0', grad_fn=<DivBackward0>)
34
35
loss1:  tensor(2.4961, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9585, device='cuda:0', grad_fn=<DivBackward0>)
36
37
loss1:  tensor(2.3630, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5148, device='cuda:0', grad_fn=<DivBackward0>)
38
39
loss1:  tensor(2.4349, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                   | 5/10000 [00:27<15:12:17,  5.48s/it]
{'loss': 1.4842, 'grad_norm': 0.23903582990169525, 'learning_rate': 1.9990000000000003e-05, 'epoch': 0.0}
40
41
loss1:  tensor(2.1220, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7685, device='cuda:0', grad_fn=<DivBackward0>)
42
43
loss1:  tensor(2.7493, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7157, device='cuda:0', grad_fn=<DivBackward0>)
44
45
loss1:  tensor(2.0186, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7331, device='cuda:0', grad_fn=<DivBackward0>)
46
47
loss1:  tensor(1.9798, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8163, device='cuda:0', grad_fn=<DivBackward0>)
48

  0%|                                                   | 6/10000 [00:33<15:19:26,  5.52s/it]
loss1:  tensor(2.2284, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9927, device='cuda:0', grad_fn=<DivBackward0>)
loss1:  tensor(2.4858, device='cuda:0', grad_fn=<DivBackward0>)
51
loss1:  tensor(2.4858, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.3953, device='cuda:0', grad_fn=<DivBackward0>)
52
53
loss1:  tensor(1.9448, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6735, device='cuda:0', grad_fn=<DivBackward0>)
54

  0%|                                                   | 7/10000 [00:38<15:04:24,  5.43s/it]
loss1:  tensor(2.2236, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8607, device='cuda:0', grad_fn=<DivBackward0>)
56
57
loss1:  tensor(2.4921, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6724, device='cuda:0', grad_fn=<DivBackward0>)
58
59
loss1:  tensor(1.9639, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7169, device='cuda:0', grad_fn=<DivBackward0>)
60
61
loss1:  tensor(2.2824, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8616, device='cuda:0', grad_fn=<DivBackward0>)
62
63
loss1:  tensor(2.0656, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                   | 8/10000 [00:43<14:54:21,  5.37s/it]
64
65
loss1:  tensor(2.5852, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7223, device='cuda:0', grad_fn=<DivBackward0>)
66
67
loss1:  tensor(2.3664, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9934, device='cuda:0', grad_fn=<DivBackward0>)
68
69
loss1:  tensor(2.1156, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.7731, device='cuda:0', grad_fn=<DivBackward0>)
70
71
loss1:  tensor(2.0688, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6125, device='cuda:0', grad_fn=<DivBackward0>)
72
73
loss1:  tensor(2.1919, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.5818, device='cuda:0', grad_fn=<DivBackward0>)
74
75
loss1:  tensor(2.0739, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7761, device='cuda:0', grad_fn=<DivBackward0>)
76
77
loss1:  tensor(1.7271, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8336, device='cuda:0', grad_fn=<DivBackward0>)
78
79
loss1:  tensor(2.0328, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 10/10000 [00:55<15:38:13,  5.63s/it]
{'loss': 1.5259, 'grad_norm': 0.3073936104774475, 'learning_rate': 1.9980000000000002e-05, 'epoch': 0.0}
80
81
loss1:  tensor(1.7643, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8307, device='cuda:0', grad_fn=<DivBackward0>)
82
83
loss1:  tensor(2.2782, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.1976, device='cuda:0', grad_fn=<DivBackward0>)
84
85
loss1:  tensor(2.1542, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 11/10000 [01:00<15:27:24,  5.57s/it]
86
87
loss1:  tensor(2.0752, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9091, device='cuda:0', grad_fn=<DivBackward0>)
88
89
loss1:  tensor(1.8715, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.0673, device='cuda:0', grad_fn=<DivBackward0>)
90
91
loss1:  tensor(2.6497, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8229, device='cuda:0', grad_fn=<DivBackward0>)
92
93
loss1:  tensor(2.6655, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8596, device='cuda:0', grad_fn=<DivBackward0>)
94
95

  0%|                                                  | 12/10000 [01:06<15:27:39,  5.57s/it]
loss2:  tensor(0.9764, device='cuda:0', grad_fn=<DivBackward0>)
96
97
loss1:  tensor(2.3005, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6817, device='cuda:0', grad_fn=<DivBackward0>)
98
99
loss1:  tensor(2.3884, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6465, device='cuda:0', grad_fn=<DivBackward0>)
100
101
loss1:  tensor(1.6991, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7460, device='cuda:0', grad_fn=<DivBackward0>)
102
103
loss1:  tensor(1.8031, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.7630, device='cuda:0', grad_fn=<DivBackward0>)
104
105
loss1:  tensor(2.4977, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5160, device='cuda:0', grad_fn=<DivBackward0>)
106
107
loss1:  tensor(2.0555, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9021, device='cuda:0', grad_fn=<DivBackward0>)
108
109
loss1:  tensor(2.3298, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9939, device='cuda:0', grad_fn=<DivBackward0>)
110
111

  0%|                                                  | 14/10000 [01:18<16:01:57,  5.78s/it]
loss2:  tensor(0.4626, device='cuda:0', grad_fn=<DivBackward0>)
112
113
loss1:  tensor(1.9305, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7380, device='cuda:0', grad_fn=<DivBackward0>)
114
115
loss1:  tensor(2.0940, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7310, device='cuda:0', grad_fn=<DivBackward0>)
116
117
loss1:  tensor(1.9347, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4533, device='cuda:0', grad_fn=<DivBackward0>)
118
119

  0%|                                                  | 15/10000 [01:24<16:12:26,  5.84s/it]
loss2:  tensor(0.8539, device='cuda:0', grad_fn=<DivBackward0>)
{'loss': 1.4712, 'grad_norm': 0.27083104848861694, 'learning_rate': 1.9970000000000004e-05, 'epoch': 0.0}
120
121
loss1:  tensor(1.7151, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6809, device='cuda:0', grad_fn=<DivBackward0>)
122
123
loss1:  tensor(2.3067, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7924, device='cuda:0', grad_fn=<DivBackward0>)
124
125
loss1:  tensor(2.2112, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.1228, device='cuda:0', grad_fn=<DivBackward0>)
126
127
loss1:  tensor(1.9398, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 16/10000 [01:29<15:57:39,  5.76s/it]
128
129
loss1:  tensor(2.2837, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6968, device='cuda:0', grad_fn=<DivBackward0>)
130
131
loss1:  tensor(2.1513, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7373, device='cuda:0', grad_fn=<DivBackward0>)
132
133
loss1:  tensor(2.3906, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8695, device='cuda:0', grad_fn=<DivBackward0>)
134
135
loss1:  tensor(2.2067, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 17/10000 [01:35<15:50:25,  5.71s/it]
136
137
loss1:  tensor(2.8050, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6633, device='cuda:0', grad_fn=<DivBackward0>)
138
139
loss1:  tensor(2.1736, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8765, device='cuda:0', grad_fn=<DivBackward0>)
140
141
loss1:  tensor(2.2797, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8535, device='cuda:0', grad_fn=<DivBackward0>)
142
143
loss1:  tensor(2.3009, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7319, device='cuda:0', grad_fn=<DivBackward0>)
144
145
loss1:  tensor(2.1005, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(1.1455, device='cuda:0', grad_fn=<DivBackward0>)
146
147
loss1:  tensor(2.0411, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(6.6328, device='cuda:0', grad_fn=<DivBackward0>)
148
149
loss1:  tensor(2.4985, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7040, device='cuda:0', grad_fn=<DivBackward0>)
150
151
loss1:  tensor(1.8850, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 19/10000 [01:45<15:08:17,  5.46s/it]
152
153
loss1:  tensor(1.9769, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6172, device='cuda:0', grad_fn=<DivBackward0>)
154
155
loss1:  tensor(1.9342, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7390, device='cuda:0', grad_fn=<DivBackward0>)
156
157
loss1:  tensor(2.6659, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7048, device='cuda:0', grad_fn=<DivBackward0>)
158
159
loss1:  tensor(1.9875, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 20/10000 [01:51<15:17:10,  5.51s/it]
{'loss': 1.6474, 'grad_norm': 0.24687862396240234, 'learning_rate': 1.9960000000000002e-05, 'epoch': 0.0}
160
161
loss1:  tensor(2.6047, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7023, device='cuda:0', grad_fn=<DivBackward0>)
162
163
loss1:  tensor(1.8203, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8099, device='cuda:0', grad_fn=<DivBackward0>)
164
165
loss1:  tensor(2.3030, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9224, device='cuda:0', grad_fn=<DivBackward0>)
166
167
loss1:  tensor(2.0325, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7646, device='cuda:0', grad_fn=<DivBackward0>)
168
169

  0%|                                                  | 21/10000 [01:57<15:13:08,  5.49s/it]
loss2:  tensor(1.4182, device='cuda:0', grad_fn=<DivBackward0>)
170
171
loss1:  tensor(2.2985, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6992, device='cuda:0', grad_fn=<DivBackward0>)
172
173
loss1:  tensor(2.7573, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5710, device='cuda:0', grad_fn=<DivBackward0>)
174
175
loss1:  tensor(1.9035, device='cuda:0', grad_fn=<DivBackward0>)

  0%|                                                  | 22/10000 [02:02<14:47:49,  5.34s/it]
176
177
loss1:  tensor(2.2103, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6825, device='cuda:0', grad_fn=<DivBackward0>)
178
179
loss1:  tensor(2.9558, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6378, device='cuda:0', grad_fn=<DivBackward0>)
180
181
loss1:  tensor(2.0877, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7699, device='cuda:0', grad_fn=<DivBackward0>)
182
183
loss1:  tensor(2.0417, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7676, device='cuda:0', grad_fn=<DivBackward0>)
184

  0%|                                                  | 23/10000 [02:07<14:43:50,  5.32s/it]
loss1:  tensor(2.2339, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7021, device='cuda:0', grad_fn=<DivBackward0>)
186
187
loss1:  tensor(2.0780, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6029, device='cuda:0', grad_fn=<DivBackward0>)
188
189
loss1:  tensor(2.0833, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4244, device='cuda:0', grad_fn=<DivBackward0>)
190
191
loss1:  tensor(1.9516, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9474, device='cuda:0', grad_fn=<DivBackward0>)
192

  0%|                                                  | 24/10000 [02:13<15:16:14,  5.51s/it]
loss1:  tensor(2.0292, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.5507, device='cuda:0', grad_fn=<DivBackward0>)
194
195
loss1:  tensor(1.8877, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8138, device='cuda:0', grad_fn=<DivBackward0>)
196
197
loss1:  tensor(2.2253, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.1509, device='cuda:0', grad_fn=<DivBackward0>)
198
199
loss1:  tensor(2.1826, device='cuda:0', grad_fn=<DivBackward0>)

loss2:  tensor(0.4509, device='cuda:0', grad_fn=<DivBackward0>)
{'loss': 1.4747, 'grad_norm': 0.25839316844940186, 'learning_rate': 1.9950000000000004e-05, 'epoch': 0.0}
200
201
loss1:  tensor(2.3113, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4337, device='cuda:0', grad_fn=<DivBackward0>)
202
203
loss1:  tensor(2.3743, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8373, device='cuda:0', grad_fn=<DivBackward0>)
204
205
loss1:  tensor(1.8818, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.3420, device='cuda:0', grad_fn=<DivBackward0>)
206
207
loss1:  tensor(2.1763, device='cuda:0', grad_fn=<DivBackward0>)

  0%|▏                                                 | 26/10000 [02:24<15:21:51,  5.55s/it]
208
209
loss1:  tensor(2.2822, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7858, device='cuda:0', grad_fn=<DivBackward0>)
210
211
loss1:  tensor(1.8359, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9208, device='cuda:0', grad_fn=<DivBackward0>)
212
213
loss1:  tensor(1.6767, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8407, device='cuda:0', grad_fn=<DivBackward0>)
214
215
loss1:  tensor(2.5744, device='cuda:0', grad_fn=<DivBackward0>)

  0%|▏                                                 | 27/10000 [02:29<15:20:53,  5.54s/it]
216
217
loss1:  tensor(0.4252, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8028, device='cuda:0', grad_fn=<DivBackward0>)
218
219
loss1:  tensor(1.8168, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4842, device='cuda:0', grad_fn=<DivBackward0>)
220
221
loss1:  tensor(2.3886, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.4663, device='cuda:0', grad_fn=<DivBackward0>)
222
223
loss1:  tensor(2.3514, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8780, device='cuda:0', grad_fn=<DivBackward0>)
224
225

  0%|▏                                                 | 28/10000 [02:35<15:13:10,  5.49s/it]
loss2:  tensor(0.6952, device='cuda:0', grad_fn=<DivBackward0>)
226
227
loss1:  tensor(2.0386, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6747, device='cuda:0', grad_fn=<DivBackward0>)
228
229
loss1:  tensor(1.8579, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.9260, device='cuda:0', grad_fn=<DivBackward0>)
230
231

  0%|▏                                                 | 29/10000 [02:40<14:42:27,  5.31s/it]
loss2:  tensor(0.7281, device='cuda:0', grad_fn=<DivBackward0>)
232
233
loss1:  tensor(2.6376, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.0175, device='cuda:0', grad_fn=<DivBackward0>)
234
235
loss1:  tensor(1.9025, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(1.1819, device='cuda:0', grad_fn=<DivBackward0>)
236
237
loss1:  tensor(2.6575, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7836, device='cuda:0', grad_fn=<DivBackward0>)
238
239
loss1:  tensor(2.3319, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.3830, device='cuda:0', grad_fn=<DivBackward0>)
{'loss': 1.4349, 'grad_norm': 0.4450761079788208, 'learning_rate': 1.9940000000000002e-05, 'epoch': 0.0}
240
241

loss1:  tensor(1.8205, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.3650, device='cuda:0', grad_fn=<DivBackward0>)
242
243
loss1:  tensor(1.7325, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.2378, device='cuda:0', grad_fn=<DivBackward0>)
244
245
loss1:  tensor(2.3322, device='cuda:0', grad_fn=<DivBackward0>)

  0%|▏                                                 | 31/10000 [02:50<14:53:43,  5.38s/it]
246
247
loss1:  tensor(2.1067, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7713, device='cuda:0', grad_fn=<DivBackward0>)
248
249
loss1:  tensor(2.2243, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7585, device='cuda:0', grad_fn=<DivBackward0>)
250
251
loss1:  tensor(1.8893, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6492, device='cuda:0', grad_fn=<DivBackward0>)
252
253
loss1:  tensor(2.5937, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.8611, device='cuda:0', grad_fn=<DivBackward0>)
254
255

loss1:  tensor(2.2142, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7828, device='cuda:0', grad_fn=<DivBackward0>)
256
257
loss1:  tensor(2.1887, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7245, device='cuda:0', grad_fn=<DivBackward0>)
258
259
loss1:  tensor(1.9691, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.6806, device='cuda:0', grad_fn=<DivBackward0>)
260
261
loss1:  tensor(2.0665, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(0.7213, device='cuda:0', grad_fn=<DivBackward0>)
262
263

  0%|▏                                                 | 33/10000 [03:01<14:50:30,  5.36s/it]
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
264
265
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
266
267
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
268
269

  0%|▏                                                 | 34/10000 [03:06<14:50:57,  5.36s/it]
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
270
271
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
272
273
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
274
275
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
276
277
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
278

  0%|▏                                                 | 35/10000 [03:12<14:47:05,  5.34s/it]
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
{'loss': 5.1517, 'grad_norm': nan, 'learning_rate': 1.9930000000000004e-05, 'epoch': 0.0}
280
281
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
282
283
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
284
285
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
286
287
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)

  0%|▏                                                 | 36/10000 [03:17<14:29:48,  5.24s/it]
288
289
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
290
291
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
292
293
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
294
295
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
296

  0%|▏                                                 | 37/10000 [03:23<15:01:28,  5.43s/it]
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
298
299
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
300
301
loss1:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)
302
303
  0%|▏                                                 | 37/10000 [03:23<15:01:28,  5.43s/it]Traceback (most recent call last):
  File "/home/jingbo/KVMemory/finetune_combine.py", line 94, in <module>
    main()
  File "/home/jingbo/KVMemory/finetune_combine.py", line 86, in main
    trainer.train()
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 2274, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 3344, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/accelerate/accelerator.py", line 2143, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1967, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2057, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/jingbo/KVMemory/finetune_combine.py", line 94, in <module>
[rank0]:     main()
[rank0]:   File "/home/jingbo/KVMemory/finetune_combine.py", line 86, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 2274, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/transformers/trainer.py", line 3344, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/accelerate/accelerator.py", line 2143, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1967, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2057, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/jingbo/anaconda3/envs/unlearning/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
loss2:  tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)